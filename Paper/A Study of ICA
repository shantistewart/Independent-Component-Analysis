

\documentclass[conference]{IEEEtran}
% \IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\argmax}{argmax}
\newcommand*{\argminl}{\argmin\limits}
\newcommand*{\argmaxl}{\argmax\limits}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A Study of Independent Component Analysis \\
}

\author{\IEEEauthorblockN{Shanti Stewart}
\IEEEauthorblockA{\textit{School of Electrical Engineering and Computer Science} \\
\textit{Oregon State University} \\
stewars2@oregonstate.edu}
}

\maketitle


\begin{abstract}
Independent Component Analysis (ICA) is a widely used data analysis technique applicable to a broad range of fields including audio signal processing, image processing, biosignal processing, and machine learning. The aim of this paper is to provide a general introduction to the theory and methods of ICA, as well as a popular and efficient ICA algorithm known as FastICA. Additionally, an example of applying FastICA to audio source separation is presented.
\end{abstract}

\begin{IEEEkeywords}
independent component analysis, ICA, FastICA, whitening
\end{IEEEkeywords}



\section{Introduction}

To motivate the topic of independent component analysis (ICA), let us consider a well-known problem in audio signal processing known as the \textit{cocktail party problem}. The setup of the problem is as follows. There is a person, a radio, and two microphones in a single room, and neither the person, radio, nor microphones are moving relative to each other. The person is talking and the radio is playing music simultaneously, and the two microphones are constantly recording. Let us denote the audio source signals of the person (speech) and the radio (music) as $ s_1(t) $ and $ s_2(t) $, and the recorded audio signals of the two microphones as $ x_1(t) $ and $ x_2(t) $. This setup is illustrated in Fig.~\ref{cocktail_fig}.

According to the physics of acoustic theory, sound adds linearly: the audio signal that is recorded in a microphone or perceived in our ear drum is the linear combination of audio signals from multiple sources. In our example, the audio signal recorded at each microphone is a linear combination of the audio signals generated by the person and the radio:
\begin{equation}
x_1(t) = a_1s_1(t) + b_1s_2(t)
\label{cocktail_1}
\end{equation}
\begin{equation}
x_2(t) = a_2s_1(t) + b_2s_2(t)
\label{cocktail_2}
\end{equation}
where $ a_i $ and $ b_i $ are constants that depends on the distance from microphone $ i $ to the source. The recorded signals $ x_1(t) $ and $ x_2(t) $ are called a \textit{linear mixture} of the source signals $ s_1(t) $ and $ s_2(t) $. For the purpose of this problem, any time delays or other complicating details are ignored.

The goal of this problem is to recover the source signals $ s_1(t) $ and $ s_2(t) $ just by observing the recorded signals $ x_1(t) $ and $ x_2(t) $. If the coefficients $ a_i $ and $ b_i $ are known, then the problem is simply a system of linear equations, which can be solved quite easily. However, if the coefficients $ a_i $ and $ b_i $ are not known, the problem becomes much more difficult. This problem is an example from a topic called \textit{blind source separation (BSS)}, which encompasses all problems whose goal is to separate mixed sources using only observed data. One approach to solve a subset of BSS problems (namely, those with linear mixture models) is independent component analysis (ICA).

\begin{figure}[htbp]
\centerline{ \includegraphics[width=\columnwidth]{Cocktail Party.png}}
\caption{Illustration of the \textit{cocktail party problem} with a person talking, a radio playing music, and two microphones recording.}
\label{cocktail_fig}
\end{figure}



\section{Problem Formulation}

This section explains the framework of ICA and formulates ICA as a general optimization problem.

\subsection{Framework}

The basic setup of ICA is the following. Let $ \boldsymbol{x} $ be an $ n $-dimensional random vector of the observed data. Each sample of $ \boldsymbol{x} $ is assumed to be drawn from an unknown joint distribution $ P(\boldsymbol{x}) $. Let $ \boldsymbol{s} $ be an $ n $-dimensional random vector of $ n $ underlying source signals $ s_i $. For the purpose of this section, the number of sources is assumed to be equal to the number of observed signals -- this assumption simplifies several steps of the problem formulation.

There are two key assumptions behind the theory of ICA: 1) each source $ s_i $ is \textit{statistically independent} from the other sources and 2) the observed data is a \textit{linear mixture} of the sources. Both assumption are reasonable approximations to the underlying physical model in many cases. The linear mixture model can be written as:
\begin{equation}
\boldsymbol{x} = \boldsymbol{As}
\label{linear_mixture}
\end{equation}
where $ \boldsymbol{A} \in \mathbb{R}^{n \times n} $ is an unknown square matrix that "mixes" the sources. $ \boldsymbol{A} $ is known as the \textit{mixing matrix}. In this section, $ \boldsymbol{A} $ is assumed to be nonsingular.

The goal of ICA is to determine an \textit{unmixing} matrix $ \boldsymbol{W} \approx \boldsymbol{A^{-1}} $, which is an approximation of the inverse of $ \boldsymbol{A} $. The unmixing matrix can then be used to estimate the sources:
\begin{equation}
\boldsymbol{\hat{s}} = \boldsymbol{Wx}
\label{source_estimation}
\end{equation}
such that $ \boldsymbol{\hat{s}} \approx \boldsymbol{s} $.

\subsection{Initial Strategy}

To solve for the unmixing matrix $ \boldsymbol{W} $, we first decompose $ \boldsymbol{A} $ by singular value decomposition (SVD):
\begin{equation}
\boldsymbol{A} = \boldsymbol{U \Sigma V^T}
\label{A_SVD}
\end{equation}
where $ \boldsymbol{U} \in \mathbb{R}^{n \times n}$ and $ \boldsymbol{V} \in \mathbb{R}^{n \times n} $ are orthogonal matrices, and $ \boldsymbol{\Sigma} \in \mathbb{R}^{n \times n} $ is a diagonal matrix of singular values. Since $ \boldsymbol{A} $ is nonsingular and therefore full-rank, $ \boldsymbol{A} $ has $ n $ nonzero singular values and $ \boldsymbol{\Sigma} $ consequently is also full-rank and nonsingular. Using the SVD of $ \boldsymbol{A} $, $ \boldsymbol{W} $ can be written as:
\[
\boldsymbol{W} = \boldsymbol{A^{-1}} = (
\boldsymbol{U \Sigma V^T})^{-1}
\]
\begin{equation}
\boldsymbol{W} = \boldsymbol{V \Sigma^{-1} U^T}
\label{W_SVD}
\end{equation}
where the expression was simplified with the fact that the inverse of an orthogonal matrix is its transpose.

The unmixing matrix $ \boldsymbol{W} $ has been decomposed into three pieces, and we can now focus on determining each of these parts separately.

\subsection{Whitening}

This subsection explains how to solve for two out of three components of the unmixing matrix $ \boldsymbol{W} $: namely, $ \boldsymbol{\Sigma^{-1}} $ and $ \boldsymbol{U^T} $.

\subsubsection{Covariance of Data}

Inspecting the covariance of the data (specifically, the auto-covariance matrix) is a logical place to start, because the covariance contains all (linear) correlations in the data, which is appropriate given that ICA is built upon a linear mixture model. Before computing the covariance, the data is centered:
\begin{equation}
\boldsymbol{x} \leftarrow \boldsymbol{x} - \overline{\boldsymbol{x}}
\label{centering}
\end{equation}
where $ \overline{\boldsymbol{x}} $ is the sample mean of $ \boldsymbol{x} $.

In order to determine $ \boldsymbol{\Sigma^{-1}} $ and $ \boldsymbol{U^T} $, we have to make an additional assumption that the sources $ \boldsymbol{s} $ are \textit{whitened}: $ \langle \boldsymbol{ss^T} \rangle = \boldsymbol{I} $. Using this assumption, the ICA linear mixture model $ \boldsymbol{x} = \boldsymbol{As} $, and the SVD $ \boldsymbol{A} = \boldsymbol{U \Sigma V^T} $, the covariance matrix of $ \boldsymbol{x} $ can be expressed as:
\[
\langle \boldsymbol{xx^T} \rangle = \langle (\boldsymbol{As}) (\boldsymbol{As})^{\boldsymbol{T}} \rangle
\]
\[
\langle \boldsymbol{xx^T} \rangle = \langle (\boldsymbol{U \Sigma V^T s}) (\boldsymbol{U \Sigma V^T s})^{\boldsymbol{T}} \rangle
\]
\[
\langle \boldsymbol{xx^T} \rangle = \langle (\boldsymbol{U \Sigma V^T s}) (\boldsymbol{s^T V \Sigma U^T}) \rangle
\]
\[
\langle \boldsymbol{xx^T} \rangle = \boldsymbol{U \Sigma V^T} \langle \boldsymbol{ss^T} \rangle  \boldsymbol{V \Sigma U^T}
\langle \boldsymbol{xx^T} \rangle
\]\[
\langle \boldsymbol{xx^T} \rangle = \boldsymbol{U \Sigma V^T} \boldsymbol{V \Sigma U^T}
\]
\begin{equation}
\langle \boldsymbol{xx^T} \rangle = \boldsymbol{U \Sigma^2 U^T}
\label{covariance}
\end{equation}
utilizing the fact that the inverse of an orthogonal matrix is its transpose. By using the assumption that the sources are whitened, the covariance of the data can be expressed independently from the sources $ \boldsymbol{s} $ and the SVD matrix $ \boldsymbol{V} $.

\subsubsection{Eigendecomposition of the Covariance}

In order to solve for $ \boldsymbol{\Sigma^{-1}} $ and $ \boldsymbol{U^T} $, we must take the eigendecomposition of the covariance matrix. Since all covariance matrices (for real-valued vectors) are real-symmetric, $ \boldsymbol{xx^T} $ must admit an eigendecomposition with orthonormal eigenvectors (also known as \textit{orthogonal diagonalization}):
\begin{equation}
\langle \boldsymbol{xx^T} \rangle = \boldsymbol{E D E^T}
\label{eigendecomposition}
\end{equation}
Since all covariance matrices are positive semi-definite (PSD), all of the eigenvalues of $ \langle \boldsymbol{xx^T} \rangle $ are nonnegative. Furthermore, all covariance matrices of uncorrelated random vectors are positive definite (PD), and consequently all of their eigenvalues are positive. Thus, $ \langle \boldsymbol{xx^T} \rangle $ has all positive eigenvalues and $ \boldsymbol{D} $ is therefore nonsingular.

Considering \eqref{covariance} and \eqref{eigendecomposition}, both equations show an orthogonal diagonalization of the covariance matrix of the data. Since diagonalizing a real-symmetric matrix with its eigenvectors is unique up to a permutation, the right hand sides of the two equations can be readily compared:
\begin{equation}
\boldsymbol{U} = \boldsymbol{E} \text{  and  } \boldsymbol{\Sigma} = \boldsymbol{D^{1/2}}
\label{U_Sigma_expressions}
\end{equation}

Looking back at \eqref{W_SVD}, the unmixing matrix $ \boldsymbol{W} $ can now be written as:
\begin{equation}
\boldsymbol{W} = \boldsymbol{V D^{-1/2} E^T}
\label{W_expression}
\end{equation}
where $ \boldsymbol{D} $ and $ \boldsymbol{E} $ are the eigenvalues and eigenvectors of the covariance matrix of the data, respectively.

\subsubsection{Relationship to Whitening}

The matrix product $ \boldsymbol{D^{-1/2}E^T} $ is known as a \textit{whitening filter}, because it \textit{whitens} the data. Whitening is a common technique in signal processing that first rotates the data to align with its orthogonal directions of maximum variance (eigenvectors of the covariance matrix), and then normalizes each axis direction to be of unit variance. Examining \eqref{source_estimation} and \eqref{W_expression}, the estimation of the sources can be written as:
\begin{equation}
\boldsymbol{\hat{s}} = \boldsymbol{V D^{-1/2} E^T x} = \boldsymbol{V x_w}
\label{source_estimation_whitening}
\end{equation}
where $ \boldsymbol{x_w} $ is a whitened version of the (centered) data $ \boldsymbol{x} $.

Now the only unknown component of the unmixing matrix $ \boldsymbol{W} $ is an orthogonal matrix $ \boldsymbol{V} $, which is addressed in the upcoming subsections.

\subsection{Measures of Statistical Independence}

To determine the final unknown rotation matrix $ \boldsymbol{V} $, we must exploit one of our key assumptions behind ICA: each source $ s_i $ is \textit{statistically independent} from the other sources.

\subsubsection{Definition of Statistical Independence}

Statistical independence is the strongest criterion of independence between random variables. It dictates that second-order and all higher-order correlations between the random variables to be zero. In comparison, uncorrelatedness requires only second-order correlations to be zero. Covariance matrices only capture second-order correlations (i.e. linear dependencies) between random variables, and therefore can only be used to manipulate second-order correlations. In the previous subsection, whitening the data  $ \boldsymbol{x} $ only removes second-order correlations, but does not remove higher-order correlations. From probability theory, a random vector $ \boldsymbol{s} = [\boldsymbol{s_1},...,\boldsymbol{s_n}]^T \in \mathbb{R}^{n} $ is statistically independent if and only if:
\begin{equation}
P(\boldsymbol{s}) = \prod_{i=1}^{n} P(\boldsymbol{s_i})
\label{independence}
\end{equation}
where $ P(\boldsymbol{s}) $ is the joint probability density of $ \boldsymbol{s} $. In words, $ \boldsymbol{s} $ is statistically independent if and only if its joint probability density can be factored as a product of the individual probability densities of its elements $ \boldsymbol{s_i} $.

\subsubsection{Multi-Information and Entropy}

In order to minimize the statistical independence of the sources $ \boldsymbol{s_i} $, we need some measure of statistical dependence/independence. A natural measure from the branch of mathematics known as information theory is called the multi-information, which is defined as:
\begin{equation}
I(\boldsymbol{s}) = \int P(\boldsymbol{s}) \log_2 \left( \frac{P(\boldsymbol{s})}{\prod_{i=1}^{n} P(\boldsymbol{s_i})} \right) d\boldsymbol{s}
\label{multi_information}
\end{equation}
The multi-information is a measure of statistical \textit{dependence}, meaning that it has larger values for higher statistical dependencies. The multi-information is always nonnegative and achieves a minimum of zero when $ \boldsymbol{s} $ is completely statistically independent. (This is readily apparent by the fact that the argument of the $ log $ equals 1 when complete statistical independence exists.)

The final step of ICA can now be expressed concisely: determine a rotation matrix $ \boldsymbol{V} $ that minimizes the multi-information of $ \boldsymbol{\hat{s}} $, where $ \boldsymbol{\hat{s}} = \boldsymbol{V x_w} $ and $ \boldsymbol{x_w} $ is a whitened version of the (centered) data $ \boldsymbol{x} $. Finding a rotation matrix that singlehandedly removes all higher-order correlations seems like a formidable task, but if the ICA assumptions are correct, then it is achievable.

The multi-information can be written as a function of \textit{entropy}. The entropy of a probability distribution is a measure of its uncertainty and is defined as:
\begin{equation}
H(s_i) = - \int P(s_i) \log_2 \left( P(s_i) \right) ds_i
\label{entropy}
\end{equation}
With a little manipulation, the multi-information can be written as:
\begin{equation}
I(\boldsymbol{s}) = \sum_{i=1}^{n} H(s_i) - H(\boldsymbol{s})
\label{multi_information_entropy}
\end{equation}

To simplify this expression further, let us substitute in $ \boldsymbol{\hat{s}} = \boldsymbol{V x_w} $:
\[
I(\boldsymbol{s}) = \sum_{i=1}^{n} H([\boldsymbol{Vx_w}]_i) - H(\boldsymbol{Vx_w})
\]
\[
I(\boldsymbol{s}) = \sum_{i=1}^{n} H([\boldsymbol{Vx_w}]_i) - \left( H(\boldsymbol{x_w}) + \log_2 \left( det(\boldsymbol{V}) \right) \right)
\]
\begin{equation}
I(\boldsymbol{s}) = \sum_{i=1}^{n} H([\boldsymbol{Vx_w}]_i) - H(\boldsymbol{x_w})
\label{multi_information_simplified}
\end{equation}
where we have utilized an expression that relates the entropy of a probability distribution that undergoes a linear transformation and the fact that the determinant of the rotation matrix $ \boldsymbol{V} $ equals 1.

With \eqref{multi_information_simplified}, we can now write ICA as an optimization problem that finds the optimal rotation matrix $ \boldsymbol{V^*} $ that minimizes the multi-information of $ \boldsymbol{\hat{s}} $:
\begin{equation}
\boldsymbol{V^*} = \argminl_{V} \sum_{i=1}^{n} H([\boldsymbol{Vx_w}]_i)
\label{ICA_optimization}
\end{equation}
In practice, computing the entropy of a finite data set (i.e. a sampled probability distribution) is difficult and generally not robust to noise, so various approximations of entropy are used, as will be seen in the next section.


\section{FastICA Algorithm}

The FastICA algorithm, developed by \textit{A. Hyvarinen} \cite{FastICA} at the Helsinki University of Technology, is a widely used algorithm for solving ICA with good efficiency and convergence properties. As with most ICA algorithms, FastICA searches for an orthogonal rotation matrix ($ \boldsymbol{V} $), such that the statistical independence of the projection of the whitened data onto this matrix is maximized. FastICA uses an approximation of negentropy as a measure of statistical independence.

\subsection{Negentropy}

Before presenting the FastICA algorithm, we must first introduce the concept of negentropy. Negentropy is defined as:
\begin{equation}
\boldsymbol{J(\boldsymbol{s})} = H(\boldsymbol{s_{gauss}}) - H(\boldsymbol{s})
\label{negentropy}
\end{equation}
where $ \boldsymbol{s_{gauss}} $ is a Gaussian random vector of the same covariance matrix of $ \boldsymbol{s} $. As can be seen from \eqref{negentropy}, negentropy is a nonnegative quantity that reaches a minimum of zero if and only if $ \boldsymbol{s} $ is Gaussian. Negentropy is a measure of nongaussianity, since it takes on larger values for random vectors that are very different from their Gaussian equivalents (same covariance matrix) and is minimized when the random vector is Gaussian. The FastICA algorithm utilizes the fact that nongaussianity can be seen to be equivalent to statistical independence and uses an approximation of negentropy in its implementation.

\subsection{FastICA for One Unit}

The overall FastICA algorithm works by finding the individual independent components separately (with an intermediate modification). The following is the FastICA algorithm for one unit (independent component):

\begin{algorithmic}
\STATE \textbf{Input}: $ \boldsymbol{x} \in \mathbb{R}^n $ = observed data
\STATE \textbf{Output}: $ \boldsymbol{b} \in \mathbb{R}^n $ = independent component of data (weight vector)
\STATE $ \boldsymbol{b} \gets $ random vector of length $ n $
\WHILE{$ \boldsymbol{b} $ is not converged}
\STATE $ \boldsymbol{b} \gets \langle \boldsymbol{x} g(\boldsymbol{b^Tx}) \rangle - \langle g'(\boldsymbol{b^Tx}) \rangle \boldsymbol{b} $
\STATE $ \boldsymbol{b} \gets \boldsymbol{b} $ / $ \|\boldsymbol{b}\| $
\ENDWHILE
\end{algorithmic}

The function $ g() $ and its derivative $ g'() $ is a hyperparameter of the algorithm, and can be experimented with to achieve better performance. \textit{A. Hyvarinen} \cite{FastICA} recommends the set of functions
\[
g(u) = tanh(u) \text{ and } g'(u) = 1 - tanh^2(u)
\]
for general-purpose applications, and states that the set of functions
\[
g(u) = ue^{-u^2/2} \text{ and } g'(u) = (1 - u^2)e^{-u^2/2}
\]
are typically more robust.

\subsection{FastICA for Multiple Units}

The one-unit algorithm outlined in the previous subsection clearly cannot be used in series to find all of the desired independent components, because the weight vector $ \boldsymbol{b} $ would likely converge to the same independent component every time. To prevent different weight vectors from converging to the same value, the projected outputs $ \boldsymbol{b_1^Tx,...,b_n^Tx} $ must be decorrelated after every iteration. The following algorithm achieves this using a decorrelation method that is very similar to the Gram-Schmidt procedure.

\begin{algorithmic}
\STATE \textbf{Inputs}: $ \boldsymbol{x} \in \mathbb{R}^n $ = observed data, $ c $ = number of desired independent components
\STATE \textbf{Output}: $ \boldsymbol{s} \in \mathbb{R}^c $ = estimated sources (whitened)
\FOR{k = 1 to c}
\STATE $ \boldsymbol{b_k} \gets $ random vector of length $ n $
\WHILE{$ \boldsymbol{b_k} $ is not converged}
\STATE $ \boldsymbol{b_k} \gets \langle \boldsymbol{x} g(\boldsymbol{b_k^T x}) \rangle - \langle g'(\boldsymbol{b_k^T x}) \rangle \boldsymbol{b_k} $
\STATE $ \boldsymbol{b_k} \gets \boldsymbol{b_k} - \sum_{j=1}^{k-1} (\boldsymbol{b_k^T b_j}) \boldsymbol{b_j} $
\STATE $ \boldsymbol{b_k} \gets \boldsymbol{b_k} $ / $ \|\boldsymbol{b_k}\| $
\ENDWHILE
\ENDFOR
\STATE $ \boldsymbol{B} \gets [\boldsymbol{b_1},...,\boldsymbol{b_c}] $
\STATE $ \boldsymbol{s} \gets \boldsymbol{B^Tx} $
\end{algorithmic}
In this algorithm $ \boldsymbol{B} $ is equivalent to the transpose of the final rotation matrix $ \boldsymbol{V^T} $. Additionally, the output of the algorithm is a whitened version of the sources $ \boldsymbol{s} $.



\section{Application: Audio Source Separation}

This section details the results of applying the FastICA algorithm to a simple audio source separation example.

\subsection{Data Sets}

FastICA was applied to three different sets of audio files \cite{audio_files}, each with two recordings (microphones) and two desired sources and a sampling frequency of 16 kHz. The first set of audio files is a recording of two different people simultaneously counting from one to ten (in two different languages), the second set is a recording of one person counting from one to ten while music is playing in the background, and the third set is a recording of two people talking (not counting) simultaneously with some light background noise.

\subsection{Methods}

The version of (multiple-unit) FastICA described in the previous section (Section II III B) was used, with contrast functions $ g(u) = tanh(u) \text{ and } g'(u) = 1 - tanh^2(u) $. Additionally, a set number of iterations (set to 100) was used for the inner loop of the algorithm, as opposed to a convergence test, for simplicity of implementation. After applying FastICA to the data, an estimate of the mean and variance was added back in to the estimated source signals. The mean of the original (uncentered) data was close to zero, so adding the mean back in did not make much of a difference. However, it was essential to add the variance back in to hear the estimated source signals (in the .WAV format, the volume of the audio depends on the magnitude of the signal values).

\subsection{Results}

Out of the three sets of audio files, the FastICA algorithm seemed to perform the best on the second set (counting and music), so the following results will be for this set of recordings.

We can visually evaluate the results of the algorithm with simple time-domain plots. Fig.~\ref{X_plot} shows a plot of the observed audio signals $ x_1(t) $ and $ x_2(t) $ in the time domain. It can be seen that the counting (from one to ten) is more apparent in the first recording $ x_1(t) $, from the roughly periodic spikes in signal value. This reflects the fact that microphone 1 is closer to the person counting, as was mentioned in the introduction section. After applying FastICA to $ x_1(t) $ and $ x_2(t) $, the estimated source audio signals are displayed in Fig.~\ref{S_plot}. One can readily see that source $ s_1(t) $ contains more information of counting than music, as can be heard in the output audio files. Additionally, the magnitude of $ s_1(t) $ is much smaller than the magnitude of $ s_2(t) $, which exemplifies one of the shortcomings of ICA.

Another way to visually evaluate the results is to examine the scatter plots of the data and estimated sources (clearly, this is possible only with 2-dimensional data). Looking at Fig.~\ref{X_scatter}, we can see that $ x_1 $ and $ x_2 $ are fairly strongly correlated; this is expected, because when a sound (from either source) increase or decreases, both microphones will reflect this change in volume somewhat similarly (since neither microphone is extremely close or far away from either source). Examining the scatter plot of the estimated sources in Fig.~\ref{S_scatter}, we can see evidence of the whitening step in ICA since no one direction has significantly more variance than the others.

Since the medium of the data is audio, the best evaluation is to actually listen to the results: the audio output files (in .WAV format) can be found at \url{https://github.com/shantistewart/Independent-Component-Analysis/tree/master/Code}\footnote{The audio files for this example are named "music\_x1.wav", "music\_x2.wav", "music\_s1.wav", "music\_s1.wav", where "x" represents the observed signals and "s" represents the estimated source signals.} As can be heard, $ s_1(t) $ contains noticeably more sound of counting than music, although the total volume is much quieter than the other audio files. And as expected, $ s_2(t) $ contains more sound of music than counting.

Clearly, these results are far from perfect: each estimated source is significantly contaminated with the other. However, this problem of blind source separation is inherently difficult, and the fact that a linear transformation of the data can partly recover the source is quite remarkable.



\begin{thebibliography}{00}

\bibitem{ICA_algorithms} A. Hyvärinen and E. Oja, “Independent Component Analysis: Algorithms and Applications,” Neural Networks, vol. 13, no. 4-5, pp. 411–430, 2000.
\bibitem{FastICA} A. Hyvarinen, “Fast and Robust Fixed-Point Algorithms for Independent Component Analysis,” IEEE Transactions on Neural Networks, vol. 10, no. 3, pp. 626–634, 1999.
\bibitem{ICA_tutorial} J. Shlens, “A Tutorial on Independent Component Analysis,” arxiv, 14-Apr-2014. [Online]. Available: https://arxiv.org/pdf/1404.2986.pdf.
\bibitem{audio_files} “Blind Source Separation of Recorded Speech and Music Signals,” Blind Source Separation: Audio Examples. [Online]. Available: https://cnl.salk.edu/~tewon/Blind/blind\_audio.html.

\end{thebibliography}



\begin{figure*}[htbp]
\centerline{ \includegraphics[width=2.0\columnwidth]{X_plot.png}}
\caption{Plot of the observed audio signals $ x_1(t) $ and $ x_2(t) $ in the time domain.}
\label{X_plot}
\end{figure*}
\begin{figure*}[htbp]
\centerline{ \includegraphics[width=2.0\columnwidth]{S_plot.png}}
\caption{Plot of the estimated source audio signals $ s_1(t) $ and $ s_2(t) $ in the time domain.}
\label{S_plot}
\end{figure*}
\begin{figure*}[htbp]
\centerline{ \includegraphics[width=2.0\columnwidth]{X_scatter.png}}
\caption{Scatter plot of the observed audio signals $ x_1 $ and $ x_2 $.}
\label{X_scatter}
\end{figure*}
\begin{figure*}[htbp]
\centerline{ \includegraphics[width=2.0\columnwidth]{S_scatter.png}}
\caption{Scatter plot of the estimated source audio signals $ s_1 $ and $ s_2 $.}
\label{S_scatter}
\end{figure*}




\end{document}
